{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Street Map Data\n",
    "\n",
    "This notebook is used to extract hiking route data from the Overpass API ([Link](https://overpass-turbo.eu/)).\n",
    "\n",
    "First, we request hiking routes from the API using Overpass QL (short for \"Overpass Query Language\").\n",
    "In OpenStreetMap, hiking routes are defined as relations. We search for relations with specific signage and the tags \"hiking routes,\" \"local walking network\",\n",
    "within an area slightly larger than Switzerland. Using \"Center\" as Output, OpenStreetMap calculates the central location of each route.\n",
    "Since the \"name\" tag is often missing, we interpolate the name by concatenating the start and end points of each hiking route.\n",
    "Finally, we retrieve the ID, name, latitude, and longitude as data points. \n",
    "\n",
    "The data is then converted into a DataFrame object, and a table is created in an SQL database (hosted on Microsoft Azure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import overpy\n",
    "import pyodbc\n",
    "import urllib\n",
    "import pymssql\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import Integer, String, Float, DATETIME, create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Overpass API with a custom URL\n",
    "api = overpy.Overpass(url=\"http://overpass.osm.ch/api/interpreter\")\n",
    "\n",
    "# Overpass query for hiking trails within Switzerland. Using 'center', we obtain the coordinates in the middle of a hiking trail\n",
    "query = \"\"\"\n",
    "[out:json];\n",
    "relation\n",
    "[\"route\"=\"hiking\"]\n",
    "//[\"name\"!~\"fixme\", i]\n",
    "[\"network\"=\"lwn\"]\n",
    "[\"osmc:symbol\"~\"yellow::yellow_diamond|red:white:red_bar|yellow:white:yellow_diamond|blue:white:blue_bar\"]\n",
    "(45.8899, 6.0872, 47.8085, 10.4921);\n",
    "out center tags;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the request\n",
    "result = api.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time and datestamp of API call to dataframe\n",
    "timestamp_apicall = pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# List to store the extracted information\n",
    "list = []\n",
    "\n",
    "# Iterate over all relations\n",
    "for relation in result.relations:\n",
    "\n",
    "    # Extract relevant data\n",
    "    name = relation.tags.get('name')\n",
    "    von = relation.tags.get('from')\n",
    "    bis = relation.tags.get('to')\n",
    "    symbol = relation.tags.get('osmc:symbol')\n",
    "    lat = getattr(relation, 'center_lat')\n",
    "    lon = getattr(relation, 'center_lon')\n",
    "    \n",
    "    dict = {    \n",
    "    'id': relation.id,\n",
    "    'name': name,\n",
    "    'symbol': symbol,\n",
    "    'von': von,\n",
    "    'bis': bis,\n",
    "    'lat': lat,\n",
    "    'lon': lon,\n",
    "    'timestamp_apicall': timestamp_apicall}\n",
    "\n",
    "    # Each tuple is now saved in the list as a new row\n",
    "    list.append(dict)\n",
    "\n",
    "# Once all data is processed, create the DataFrame\n",
    "df_wanderwege = pd.DataFrame(list)\n",
    "\n",
    "# Convert lat and lon to numeric, timestamp to datetime\n",
    "df_wanderwege['lat'] = pd.to_numeric(df_wanderwege['lat'], errors='coerce')\n",
    "df_wanderwege['lon'] = pd.to_numeric(df_wanderwege['lon'], errors='coerce')\n",
    "df_wanderwege['timestamp_apicall'] = pd.to_datetime(df_wanderwege['timestamp_apicall'], errors='coerce')\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_wanderwege.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing Data:\")\n",
    "print(df_wanderwege.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows with any missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wanderwege = df_wanderwege.dropna(subset=[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_wanderwege.iterrows():\n",
    "    if pd.isnull(row[\"name\"]) and not pd.isnull(row[\"von\"]) and not pd.isnull(row[\"bis\"]):\n",
    "        df_wanderwege.at[index, \"name\"] = f\"{row['von']} - {row['bis']}\"\n",
    "\n",
    "# Resultierender DataFrame anzeigen\n",
    "print(df_wanderwege)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying duplicate rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDuplicate rows:\")\n",
    "print(df_wanderwege.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying similar rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz, process\n",
    "import pandas as pd\n",
    "\n",
    "# Namen auf Ähnlichkeit prüfen\n",
    "threshold = 80  # Ähnlichkeitsschwelle (z. B. 80%)\n",
    "similar_names = []\n",
    "\n",
    "# Iteriere über alle Namen\n",
    "for index, name in enumerate(df_wanderwege[\"name\"]):\n",
    "    for other_index, other_name in enumerate(df_wanderwege[\"name\"]):\n",
    "        if index != other_index:  # Nicht mit sich selbst vergleichen\n",
    "            score = fuzz.ratio(name, other_name)\n",
    "            if score >= threshold:  # Wenn Ähnlichkeit über Schwelle liegt\n",
    "                similar_names.append((name, other_name, score))\n",
    "\n",
    "# Ähnliche Namen anzeigen\n",
    "print(\"Ähnliche Namen:\")\n",
    "for name1, name2, score in similar_names:\n",
    "    print(f\"'{name1}' und '{name2}' haben eine Ähnlichkeit von {score}%.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove similar Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wanderwege = df_wanderwege[~df_wanderwege[\"name\"].str.contains(\"fixme\", case=False, na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze latitude and loninude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame für die relevanten Spalten vorbereiten\n",
    "df_melted = df_wanderwege[[\"lat\", \"lon\"]].melt(var_name=\"Variable\", value_name=\"Wert\")\n",
    "\n",
    "# Boxplot zeichnen\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=\"Variable\", y=\"Wert\", data=df_melted)\n",
    "plt.title(\"Boxplot der Verteilung von Latitude und Longitude\")\n",
    "plt.ylabel(\"Wert\")\n",
    "plt.xlabel(\"Variable\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove invalid latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prüfen und Entfernen von Zeilen mit negativen Werten in 'lan' und 'lot'\n",
    "df_wanderwege = df_wanderwege[(df_wanderwege[\"lat\"] > 0) & (df_wanderwege[\"lon\"] > 0)]\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "print(df_wanderwege)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optional: Store data in csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wanderwege.to_csv(\"../data/processed/overpass.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from config/db_config.json\n",
    "with open('../config/db_config.json', 'r') as f:\n",
    "    db_config = json.load(f)\n",
    "\n",
    "# Get database credentials\n",
    "server = db_config['server']\n",
    "database = db_config['database']\n",
    "db_user = db_config['db_user']\n",
    "db_password = db_config['db_password']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create empty SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table if it doesn't exist\n",
    "table_name = \"OVRP_HikingRoutes\"\n",
    "query = f\"\"\"\n",
    "    IF OBJECT_ID(N'dbo.{table_name}', N'U') IS NULL\n",
    "    BEGIN\n",
    "        CREATE TABLE {table_name} (\n",
    "            id                      INT         NOT NULL,\n",
    "            name                    VARCHAR(255) NULL,\n",
    "            von                     VARCHAR(255) NULL,\n",
    "            bis                     VARCHAR(255) NULL,\n",
    "            lat                     FLOAT       NOT NULL,\n",
    "            lon                     FLOAT       NOT NULL,\n",
    "            symbol                  VARCHAR(255) NULL,\n",
    "            timestamp_apicall       DATETIME    NULL,\n",
    "            PRIMARY KEY (id)\n",
    "        );\n",
    "    END\n",
    "    \"\"\"\n",
    "\n",
    "conn = pymssql.connect(server, db_user, db_password, database)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(query)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection string for SQLAlchemy\n",
    "connection_string = f\"mssql+pymssql://{db_user}:{db_password}@{server}/{database}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Ingest data to tabledatabase table\n",
    "df_wanderwege.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "print(\"DataFrame erfolgreich in die MSSQL-Datenbank geladen!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "www",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
